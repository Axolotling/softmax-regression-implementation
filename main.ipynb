{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_with_same_order(a: np.ndarray, axis=None):\n",
    "    return a[np.sort(np.unique(a, return_index=True, axis=axis)[1])]\n",
    "\n",
    "def convert_to_one_hot(a: Union[np.ndarray, list]):\n",
    "    if isinstance(a, list):\n",
    "        a = np.array(a)\n",
    "    return np.vstack([a == v for v in unique_with_same_order(a, axis=0)]).T.astype(np.float64)\n",
    "\n",
    "def standard_scaling(X):\n",
    "    return (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "def train_test_split(X = None, population_size: int = None, split: float = 0.2):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    X: np.ndarray or pd.DataFrame passed to infer population_size\n",
    "    population_size: can be passed instead of X\n",
    "    split: fraction of dataset that goes into test set\n",
    "    \"\"\"\n",
    "    if X is not None:\n",
    "        population_size = X.shape[0]\n",
    "        \n",
    "    shuffled_index = np.random.permutation(np.arange(population_size))\n",
    "    test_size = int(population_size * split)\n",
    "    test_index = shuffled_index[:test_size]\n",
    "    train_index = shuffled_index[test_size:]\n",
    "    return train_index, test_index\n",
    "\n",
    "def softmax(a, rowwise=True):\n",
    "    if rowwise:\n",
    "        return (np.exp(a).T / np.exp(a).T.sum(0)).T\n",
    "    else:\n",
    "        return np.exp(a) / np.exp(a).sum()\n",
    "    \n",
    "def predict_proba(params, X):\n",
    "    return softmax(params.dot(X.T).T)\n",
    "    \n",
    "def predict(params, X):\n",
    "    return np.argmax(\n",
    "        predict_proba(params, X), \n",
    "        axis=1)\n",
    "\n",
    "def cross_entropy(y, y_hat, X = None, gradient: bool = False):\n",
    "    \"\"\"\n",
    "    params\n",
    "    y: ground_truth labels encoded as 1-hot\n",
    "    y_hat: same shape as y, but with probabilities of classes instead of ground_truth\n",
    "    X: required only if gradient set to True \n",
    "    gradient: True if function should return a gradient of cross_entropy\n",
    "    \"\"\"\n",
    "    if gradient:\n",
    "        if X is None:\n",
    "            raise ValueError(\"X parameter cannot be None when calculating gradient\")\n",
    "        return ((y_hat - y).T @ X) / y_hat.shape[0]\n",
    "    else:\n",
    "        return -(1/y.shape[0]) * np.multiply(y, np.log(y_hat)).sum()\n",
    "    \n",
    "def accuracy(output, target):\n",
    "    return np.sum(output == target) / target.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegressionClassifier:\n",
    "    def __init__(self):\n",
    "        self.params: np.ndarray = None\n",
    "        self.classes: np.ndarray = None\n",
    "        \n",
    "    def fit(self, X, y, X_val, y_val, learning_rate: float, n_epochs: int = 10):\n",
    "        n_params = X.shape[1]\n",
    "        self.classes = unique_with_same_order(y, axis=0)\n",
    "        self._init_params(self.classes.shape[0], n_params)\n",
    "        y_oh = convert_to_one_hot(y)\n",
    "        \n",
    "        for epoch_i in range(n_epochs):\n",
    "            y_hat = self.predict_proba(X)\n",
    "            loss = cross_entropy(y_oh, y_hat)\n",
    "            gradient = cross_entropy(y_oh, y_hat, X, gradient=True)\n",
    "            self.params -= learning_rate * gradient\n",
    "            \n",
    "    def _init_params(self, n_classes, n_params):\n",
    "        self.params = np.random.normal(size=(n_classes, n_params))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.params is None:\n",
    "            raise AssertionError(\"You cannot predict before you fit the model\")\n",
    "        return softmax(self.params.dot(X.T).T)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.classes[np.argmax(\n",
    "            self.predict_proba(X), \n",
    "            axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target_names[iris.target]\n",
    "\n",
    "population_size = X.shape[0]\n",
    "X_scaled = standard_scaling(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.2 \n",
    "train_index, val_index = train_test_split(X_scaled, split=0.2)\n",
    "X_train, y_train = X_scaled[train_index], y[train_index]\n",
    "X_val, y_val = X_scaled[val_index], y[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_cls = SoftmaxRegressionClassifier()\n",
    "sr_cls.fit(X_train, y_train, X_val, y_val, learning_rate=0.05, n_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy: 0.8\n",
      "train_accuracy: 0.833\n",
      "bias error: 0.167\n",
      "variance error: 0.033\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = accuracy(sr_cls.predict(X_val), y_val)\n",
    "train_accuracy = accuracy(sr_cls.predict(X_train), y_train)\n",
    "print('val_accuracy:', val_accuracy.round(3))\n",
    "print('train_accuracy:', train_accuracy.round(3))\n",
    "bias = 1 - train_accuracy\n",
    "variance = 1 - val_accuracy - bias\n",
    "print('bias error:', bias.round(3))\n",
    "print('variance error:', variance.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
